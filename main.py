# -*- coding: utf-8 -*-
"""Main.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dGigiVszSR8YMcTFU4JSLZUYOVlw_xMK

# **Topic: Predicting Uber Ride Cancellations**

This project, Predicting Uber Ride Cancellations, uses a Kaggle dataset of ~148k rides from India/NCR to uncover why trips get cancelled. By cleaning data, exploring patterns, and building machine learning models, we aim to spot key factors behind cancellations and predict them before they happen.

> *The insights gained can help optimize platform operations, enhance rider experience, and support smarter decision-making for both business and customers.*

##**Data**


---


The dataset is taken from Kaggle and contains ~148,000 Uber rides from the India/NCR region. Dataset Link: https://www.kaggle.com/datasets/yashdevladdha/uber-ride-analytics-dashboard/data?select=ncr_ride_bookings.csv

##**Evaluation**


---


We will evaluate multiple machine learning models (Logistic Regression, Random Forest, XGBoost, CatBoost) using metrics like ROC-AUC, F1-score, and precision-recall balance. Our goal is to build a model that can achieve strong predictive performance, ideally exceeding 75% accuracy and robust recall for cancellations

| **Field**                             | **Description**                                                                 |
| ------------------------------------- | ------------------------------------------------------------------------------- |
| **Date**                              | Date of the booking                                                             |
| **Time**                              | Time of the booking                                                             |
| **Booking ID**                        | Unique identifier for each ride booking                                         |
| **Booking Status**                    | Status of booking (Completed, Cancelled by Customer, Cancelled by Driver, etc.) |
| **Customer ID**                       | Unique identifier for customers                                                 |
| **Vehicle Type**                      | Type of vehicle (Go Mini, Go Sedan, Auto, eBike/Bike, UberXL, Premier Sedan)    |
| **Pickup Location**                   | Starting location of the ride                                                   |
| **Drop Location**                     | Destination location of the ride                                                |
| **Avg VTAT**                          | Average time for driver to reach pickup location (in minutes)                   |
| **Avg CTAT**                          | Average trip duration from pickup to destination (in minutes)                   |
| **Cancelled Rides by Customer**       | Flag indicating customer-initiated cancellation                                 |
| **Reason for Cancelling by Customer** | Reason provided by customer for cancellation                                    |
| **Cancelled Rides by Driver**         | Flag indicating driver-initiated cancellation                                   |
| **Driver Cancellation Reason**        | Reason provided by driver for cancellation                                      |
| **Incomplete Rides**                  | Flag indicating incomplete ride                                                 |
| **Incomplete Rides Reason**           | Reason for incomplete rides                                                     |
| **Booking Value**                     | Total fare amount for the ride                                                  |
| **Ride Distance**                     | Distance covered during the ride (in km)                                        |
| **Driver Ratings**                    | Rating given to driver (1â€“5 scale)                                              |
| **Customer Rating**                   | Rating given by customer (1â€“5 scale)                                            |
| **Payment Method**                    | Method used for payment (UPI, Cash, Credit Card, Uber Wallet, Debit Card)       |

###Load the Dataset
"""

# Load
from google.colab import files
uploaded = files.upload()

# display head
import pandas as pd

df = pd.read_csv('ncr_ride_bookings.csv')
display(df.head())

"""### Checking Data types and Summary"""

import pandas as pd
from IPython.display import display

def missing_table(df: pd.DataFrame) -> pd.DataFrame:
    m = df.isna().sum()
    pct = (m / len(df) * 100).round(2)
    out = (
        pd.DataFrame({"missing": m, "missing_%": pct})
        .query("missing > 0")
        .sort_values("missing_%", ascending=False)
        .astype({"missing": "int"})
    )
    return out

def numeric_profile(df: pd.DataFrame) -> pd.DataFrame:
    num = df.select_dtypes(include="number")
    if num.empty:
        return pd.DataFrame()
    prof = num.describe().T
    prof["zeros_%"] = (num.eq(0).sum() / len(df) * 100).round(2)
    prof["negatives_%"] = (num.lt(0).sum() / len(df) * 100).round(2)
    return prof

def categorical_profile(df: pd.DataFrame) -> pd.DataFrame:
    cat = df.select_dtypes(exclude="number")
    if cat.empty:
        return pd.DataFrame()
    desc = cat.describe().T  # shows count, unique, top, freq
    # helpful for high-cardinality detection
    desc["unique_ratio_%"] = (desc["unique"] / len(df) * 100).round(2)
    return desc[["count", "unique", "unique_ratio_%", "top", "freq"]]

def constant_columns(df: pd.DataFrame):
    const = [c for c in df.columns if df[c].nunique(dropna=False) <= 1]
    return const


print("Shape:", df.shape)
print("\nColumn dtypes & memory:")
df.info(memory_usage="deep")

print("\nMissing values (sorted):")
mt = missing_table(df)
display(mt if not mt.empty else pd.DataFrame({"missing": [], "missing_%": []}))

print("\nNumeric profile:")
display(numeric_profile(df))

print("\nCategorical profile:")
display(categorical_profile(df))

# Duplicates (overall and by a key if present)
print("\nDuplicate rows:", int(df.duplicated().sum()))
if "Booking ID" in df.columns:
    dup_keys = df["Booking ID"].duplicated().sum()
    print("Duplicate Booking IDs:", int(dup_keys))

# Constant signals
const = constant_columns(df)
print("Constant columns:", const if const else "None")

"""## We have 1233 identified Duplicate  Booking IDs therefore we have decided to keep only one record per booking

### Removing Duplicates
"""

# Inspecting what is duplicated?
# Which IDs repeat and how many times?
dup_counts = (df['Booking ID']
              .value_counts()
              .loc[lambda s: s > 1]
              .sort_values(ascending=False))
print("Duplicated IDs:", dup_counts.shape[0])
display(dup_counts.head(10))

# Peek at a few duplicated IDs
sample_ids = dup_counts.index[:5]
display(df[df['Booking ID'].isin(sample_ids)]
        .sort_values(['Booking ID','Date','Time'])
        .head(50))

# ensure datetime
df['Date'] = pd.to_datetime(df['Date'], errors='coerce')
df['Time'] = pd.to_datetime(df['Time'], errors='coerce').dt.time

# build a sortable timestamp (if Time is real string clock, parse to datetime first)
df['_ts'] = pd.to_datetime(df['Date'].astype(str) + ' ' + df['Time'].astype(str),
                           errors='coerce')

# keep the last (latest) row per Booking ID
df = (df.sort_values('_ts')
              .drop_duplicates(subset=['Booking ID'], keep='last')
              .drop(columns=['_ts']))

print("\nDuplicate rows:", int(df.duplicated().sum()))
if "Booking ID" in df.columns:
    dup_keys = df["Booking ID"].duplicated().sum()
    print("Duplicate Booking IDs:", int(dup_keys))

"""Feature Engineering"""

# Convert Date and Time to datetime
df['Date'] = pd.to_datetime(df['Date'], errors='coerce')
df['Time'] = pd.to_datetime(df['Time'], format='%H:%M:%S', errors='coerce').dt.time

# Combine Date and Time into a single datetime column
df['datetime'] = pd.to_datetime(df['Date'].astype(str) + ' ' + df['Time'].astype(str), errors='coerce')

# Extract temporal features
df['hour'] = df['datetime'].dt.hour
df['day'] = df['datetime'].dt.day
df['month'] = df['datetime'].dt.month
df['weekday'] = df['datetime'].dt.dayofweek  # Monday=0, Sunday=6
df['is_weekend'] = df['weekday'].isin([5, 6])

# Check result
df[['datetime', 'hour', 'day', 'month', 'weekday', 'is_weekend']].head()

"""Handling null values"""

# Create binary flags from cancellation/incomplete columns
df['is_cancelled_customer'] = df['Cancelled Rides by Customer'].notnull()
df['is_cancelled_driver'] = df['Cancelled Rides by Driver'].notnull()
df['is_incomplete'] = df['Incomplete Rides'].notnull()

# Create a flag for missing ratings and booking values
df['missing_driver_rating'] = df['Driver Ratings'].isnull()
df['missing_customer_rating'] = df['Customer Rating'].isnull()
df['missing_booking_value'] = df['Booking Value'].isnull()
df['missing_payment_method'] = df['Payment Method'].isnull()

# Optional: Fill CTAT and VTAT with median if needed
df['Avg VTAT'] = df['Avg VTAT'].fillna(df['Avg VTAT'].median())
df['Avg CTAT'] = df['Avg CTAT'].fillna(df['Avg CTAT'].median())

# Optional: Fill Ratings and Booking Value with zeros if needed (for modeling)
# df['Driver Ratings'].fillna(0, inplace=True)
# df['Customer Rating'].fillna(0, inplace=True)
# df['Booking Value'].fillna(0, inplace=True)

# Check new flags
df[['is_cancelled_customer', 'is_cancelled_driver', 'is_incomplete',
    'missing_driver_rating', 'missing_customer_rating',
    'missing_booking_value', 'missing_payment_method']].sum()

"""Target Variable Creation"""

# Target variable
df['target_customer_cancelled'] = df['is_cancelled_customer'].astype(int)

# Quick target distribution check
df['target_customer_cancelled'].value_counts(normalize=True)

"""### EDA"""

import seaborn as sns
import matplotlib.pyplot as plt

# Set style
sns.set(style="whitegrid")

# 1. Cancellation Rate by Vehicle Type
plt.figure(figsize=(10, 5))
sns.barplot(data=df, x='Vehicle Type', y='target_customer_cancelled')
plt.title("Customer Cancellation Rate by Vehicle Type")
plt.xticks(rotation=45)
plt.show()

# 2. Cancellation Rate by Hour
plt.figure(figsize=(10, 5))
sns.barplot(data=df, x='hour', y='target_customer_cancelled')
plt.title("Customer Cancellation Rate by Hour of Day")
plt.show()

# 3. Cancellation Rate by Day of Week
plt.figure(figsize=(10, 5))
sns.barplot(data=df, x='weekday', y='target_customer_cancelled')
plt.title("Customer Cancellation Rate by Day of Week")
plt.xticks(ticks=range(7), labels=["Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun"])
plt.show()

# 4. Driver Rating vs Cancellation
plt.figure(figsize=(10, 5))
sns.boxplot(data=df, x='target_customer_cancelled', y='Driver Ratings')
plt.title("Driver Ratings vs Customer Cancellation")
plt.xticks([0, 1], ['Not Cancelled', 'Cancelled'])
plt.show()

# 1. Top 10 Pickup Locations by Count
top_pickups = df['Pickup Location'].value_counts().nlargest(10).index
df_top_pickups = df[df['Pickup Location'].isin(top_pickups)]

plt.figure(figsize=(12, 6))
sns.barplot(data=df_top_pickups, x='Pickup Location', y='target_customer_cancelled')
plt.title("Cancellation Rate by Top 10 Pickup Locations")
plt.xticks(rotation=45)
plt.show()

# 2. Trend of Cancellations Over Time (Daily)
df_daily = df.groupby('Date')['target_customer_cancelled'].mean().reset_index()

plt.figure(figsize=(14, 6))
sns.lineplot(data=df_daily, x='Date', y='target_customer_cancelled')
plt.title("Daily Customer Cancellation Rate Over Time")
plt.ylabel("Cancellation Rate")
plt.xlabel("Date")
plt.show()

# 3. Optional: Heatmap of Hour vs Day of Week
heatmap_data = df.pivot_table(
    index='hour',
    columns='weekday',
    values='target_customer_cancelled',
    aggfunc='mean'
)
plt.figure(figsize=(10, 6))
sns.heatmap(heatmap_data, cmap="YlGnBu", annot=True, fmt=".2f")
plt.title("Cancellation Rate by Hour and Day of Week")
plt.ylabel("Hour of Day")
plt.xlabel("Day of Week (0=Mon, 6=Sun)")
plt.show()

from sklearn.preprocessing import LabelEncoder

# 1. Vehicle Type One-Hot Encoding
df = pd.get_dummies(df, columns=['Vehicle Type'], prefix='vehicle', drop_first=True)

# 2. Encode Top 10 Pickup Locations
top_pickups = df['Pickup Location'].value_counts().nlargest(10).index
df['pickup_encoded'] = df['Pickup Location'].apply(lambda x: x if x in top_pickups else 'Other')
df = pd.get_dummies(df, columns=['pickup_encoded'], prefix='pickup', drop_first=True)

# 3. Encode Top 10 Drop Locations
top_drops = df['Drop Location'].value_counts().nlargest(10).index
df['drop_encoded'] = df['Drop Location'].apply(lambda x: x if x in top_drops else 'Other')
df = pd.get_dummies(df, columns=['drop_encoded'], prefix='drop', drop_first=True)

# 4. Customer Booking Frequency
cust_counts = df['Customer ID'].value_counts().to_dict()
df['customer_total_bookings'] = df['Customer ID'].map(cust_counts)

# 5. Replace NaNs in numeric columns with placeholder or leave as-is
# (Let tree models handle missing)
num_cols = ['Avg VTAT', 'Avg CTAT', 'Driver Ratings', 'Customer Rating']
# df[num_cols] = df[num_cols].fillna(-1)
# 6. Drop columns not needed for modeling
drop_cols = [
    'Booking ID', 'Customer ID', 'Pickup Location', 'Drop Location',
    'Cancelled Rides by Customer', 'Reason for cancelling by Customer',
    'Cancelled Rides by Driver', 'Driver Cancellation Reason',
    'Incomplete Rides', 'Incomplete Rides Reason', 'Date', 'Time', 'datetime'
]
df.drop(columns=drop_cols, inplace=True)

# 7. Final feature list
features = [col for col in df.columns if col != 'target_customer_cancelled']
target = 'target_customer_cancelled'

print(f"Total Features for Modeling: {len(features)}")

from sklearn.utils.class_weight import compute_class_weight
import numpy as np

# Compute class weights using sklearn
class_weights = compute_class_weight(
    class_weight='balanced',
    classes=np.array([0, 1]), # Convert list to numpy array
    y=df['target_customer_cancelled']
)

weights = dict(enumerate(class_weights))
print("Class Weights:", weights)

import numpy as np

# Combine X and y temporarily
temp_df = df.copy()
temp_df['target'] = df[target]

# Select only numeric columns
numeric_cols = temp_df.select_dtypes(include=[np.number]).columns

# Compute correlation with target
correlations = temp_df[numeric_cols].corr()['target'].sort_values(key=abs, ascending=False)

# Display top correlations (excluding target itself)
print("ðŸ“Œ Top Correlated Features with Target:\n")
print(correlations.drop('target'))

"""### Logistic Regression with Class Weights"""

from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, roc_auc_score

# Drop target and keep only numeric features
clean_features = df.select_dtypes(include=['number']).columns.drop('target_customer_cancelled')

# Setup X and y
X = df[clean_features]
y = df['target_customer_cancelled']

# Impute missing values
imputer = SimpleImputer(strategy='median')
X_imputed = imputer.fit_transform(X)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X_imputed, y, test_size=0.2, stratify=y, random_state=42
)

# Train Logistic Regression
logreg = LogisticRegression(class_weight='balanced', max_iter=1000)
logreg.fit(X_train, y_train)

# Predict
y_pred = logreg.predict(X_test)
y_prob = logreg.predict_proba(X_test)[:, 1]

# Evaluate
print("ðŸŽ¯ ROC AUC Score:", roc_auc_score(y_test, y_prob))
print("\nðŸ“‹ Classification Report:\n", classification_report(y_test, y_pred, digits=3))

"""### Logistic Regression - with Hyper Parameter Tuning"""

# ----- Logistic Regression: Hyperparameter Tuning -----
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
import numpy as np

# Base model (balanced to handle class imbalance)
lr_base = LogisticRegression(
    class_weight='balanced',
    max_iter=2000,
    solver='lbfgs',  # safe with L2
    n_jobs=-1
)

# Search space (strength of regularization)
lr_param_dist = {
    "C": np.logspace(-3, 2, 30)  # 0.001 .. 100
}

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

lr_search = RandomizedSearchCV(
    estimator=lr_base,
    param_distributions=lr_param_dist,
    n_iter=25,
    scoring="roc_auc",
    cv=cv,
    random_state=42,
    n_jobs=-1,
    verbose=0
)

print("\n[LR] Tuning...")
lr_search.fit(X_train, y_train)
best_lr = lr_search.best_estimator_
print("[LR] Best params:", lr_search.best_params_)
print("[LR] Best CV AUC:", round(lr_search.best_score_, 4))

# Evaluate on test
y_prob_lr = best_lr.predict_proba(X_test)[:, 1]
y_pred_lr = best_lr.predict(X_test)
print("[LR] Test ROC-AUC:", roc_auc_score(y_test, y_prob_lr))
print("[LR] Confusion Matrix:\n", confusion_matrix(y_test, y_pred_lr))
print("[LR] Classification Report:\n", classification_report(y_test, y_pred_lr, digits=3))

"""### Random Forest with Class Weights"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, roc_auc_score
from sklearn.impute import SimpleImputer

# Define cleaned features
leaky_features = [
    'Avg VTAT', 'Ride Distance',
    'Booking Value', 'Customer Rating', 'Driver Ratings',
    'Booking Status', 'Payment Method',
    'missing_booking_value', 'missing_payment_method',
    'missing_driver_rating', 'missing_customer_rating',
    'is_cancelled_customer', 'is_cancelled_driver', 'is_incomplete'
]
clean_features = [f for f in features if f not in leaky_features]

# Prepare data
X = df[clean_features]
y = df['target_customer_cancelled']

# Impute missing values
imputer = SimpleImputer(strategy='median')
X_imputed = imputer.fit_transform(X)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X_imputed, y, test_size=0.2, stratify=y, random_state=42
)
# Train Random Forest with balanced class weights
rf = RandomForestClassifier(
    n_estimators=100,
    class_weight='balanced',
    random_state=42,
    n_jobs=-1
)
rf.fit(X_train, y_train)

# Predict & Evaluate
y_pred = rf.predict(X_test)
y_prob = rf.predict_proba(X_test)[:, 1]

print("ðŸŽ¯ ROC AUC Score:", roc_auc_score(y_test, y_prob))
print("\nðŸ“‹ Classification Report:\n", classification_report(y_test, y_pred, digits=3))

"""### Random Forest wth Hyper Parameter Tuning - RandomizedSearchCV"""

# Import required libraries
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import RandomizedSearchCV
import numpy as np

# 1. Create the base model with defaults
rf = RandomForestClassifier(class_weight="balanced", random_state=42, n_jobs=-1)

# 2. Define the parameter space to search
param_dist = {
    "n_estimators": [100, 200, 300, 400],   # number of trees
    "max_depth": [None, 5, 10, 15],         # how deep each tree can grow
    "min_samples_split": [2, 5, 10],        # min samples to split a node
    "min_samples_leaf": [1, 2, 4],          # min samples at a leaf
    "max_features": ["sqrt", "log2"]        # number of features considered
}

# 3. Setup the randomized search
rf_random = RandomizedSearchCV(
    estimator=rf,
    param_distributions=param_dist,
    n_iter=10,              # try 10 random combinations only
    scoring="roc_auc",      # evaluate by ROC-AUC
    cv=3,                   # 3-fold cross-validation
    verbose=1,
    random_state=42,
    n_jobs=-1
)

# 4. Run the search
rf_random.fit(X_train, y_train)

# 5. Print best settings and score
print("Best Random Forest params:", rf_random.best_params_)
print("Best CV AUC:", rf_random.best_score_)

"""### RandomizedSearchCV on Test Data"""

# --- Evaluate tuned RF (from RandomizedSearchCV) on TEST set ---
from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix, precision_recall_fscore_support, accuracy_score
import pandas as pd

# The best tuned model found by CV
best_rf = rf_random.best_estimator_

# Predict on the hold-out TEST set
y_prob_tuned = best_rf.predict_proba(X_test)[:, 1]
y_pred_tuned = best_rf.predict(X_test)

print("[RF tuned] Test ROC-AUC:", roc_auc_score(y_test, y_prob_tuned))
print("[RF tuned] Confusion Matrix:\n", confusion_matrix(y_test, y_pred_tuned))
print("[RF tuned] Classification Report:\n", classification_report(y_test, y_pred_tuned, digits=3))

# --- If you also have a baseline RF model (non-tuned) called 'rf' already fitted ---
# If not fitted, do: rf.fit(X_train, y_train)
y_prob_base = rf.predict_proba(X_test)[:, 1]
y_pred_base = rf.predict(X_test)

# Build a simple comparison table (focus on minority class=1)
def metric_row(name, y_true, y_pred, y_prob):
    prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=None, labels=[1])
    return {
        "Model": name,
        "Test ROC-AUC": roc_auc_score(y_true, y_prob),
        "Accuracy": accuracy_score(y_true, y_pred),
        "Precision (class 1)": prec[0],
        "Recall (class 1)": rec[0],
        "F1 (class 1)": f1[0],
    }

rows = [
    metric_row("RF baseline", y_test, y_pred_base, y_prob_base),
    metric_row("RF tuned (RandomizedSearchCV)", y_test, y_pred_tuned, y_prob_tuned),
]
cmp_df = pd.DataFrame(rows).sort_values("Test ROC-AUC", ascending=False).reset_index(drop=True)
cmp_df

"""### XGBoost Model with Imbalance Handling"""

import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
from sklearn.metrics import classification_report, roc_auc_score

# Clean feature set (reuse from Random Forest)
leaky_features = [
    'Avg VTAT', 'Ride Distance',
    'Booking Value', 'Customer Rating', 'Driver Ratings',
    'Booking Status', 'Payment Method',
    'missing_booking_value', 'missing_payment_method',
    'missing_driver_rating', 'missing_customer_rating',
    'is_cancelled_customer', 'is_cancelled_driver', 'is_incomplete'
]
clean_features = [f for f in features if f not in leaky_features]

X = df[clean_features]
y = df['target_customer_cancelled']

# Impute missing values
imputer = SimpleImputer(strategy='median')
X_imputed = imputer.fit_transform(X)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X_imputed, y, test_size=0.2, stratify=y, random_state=42
)
# Train XGBoost model
xgb_model = xgb.XGBClassifier(
    objective='binary:logistic',
    eval_metric='auc',
    scale_pos_weight=13.29,
    use_label_encoder=False,
    random_state=42,
    n_jobs=-1
)

xgb_model.fit(X_train, y_train)

# Predict & Evaluate
y_pred = xgb_model.predict(X_test)
y_prob = xgb_model.predict_proba(X_test)[:, 1]

print("ðŸŽ¯ ROC AUC Score:", roc_auc_score(y_test, y_prob))
print("\nðŸ“‹ Classification Report:\n", classification_report(y_test, y_pred, digits=3))

"""### XG Boost Hypertuning using Optuna"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install optuna

import optuna
from sklearn.model_selection import cross_val_score
import xgboost as xgb

def xgb_objective(trial):
    # Suggest hyperparameters
    n_estimators = trial.suggest_int("n_estimators", 200, 600)
    max_depth = trial.suggest_int("max_depth", 3, 10)
    learning_rate = trial.suggest_float("learning_rate", 0.01, 0.2)
    subsample = trial.suggest_float("subsample", 0.6, 1.0)
    colsample_bytree = trial.suggest_float("colsample_bytree", 0.6, 1.0)

    # Define scale_pos_weight using the value from the 'weights' dictionary
    scale_pos_weight = weights[1]

    xgb_clf = xgb.XGBClassifier(
        objective="binary:logistic",
        eval_metric="auc",
        tree_method="hist",
        random_state=42,
        n_jobs=-1,
        scale_pos_weight=scale_pos_weight,
        n_estimators=n_estimators,
        max_depth=max_depth,
        learning_rate=learning_rate,
        subsample=subsample,
        colsample_bytree=colsample_bytree
    )

    score = cross_val_score(xgb_clf, X_train, y_train, cv=3, scoring="roc_auc").mean()
    return score

# Run optimization for 20 trials
study = optuna.create_study(direction="maximize")
study.optimize(xgb_objective, n_trials=20)

print("Best XGB params:", study.best_trial.params)
print("Best CV AUC:", study.best_value)

"""### XGBoost with OPTUNA but now for TEST DATA WITH COMAPRISON"""

# ===== Evaluate Optuna-tuned XGB on TEST + compare to your existing baseline =====
import numpy as np, pandas as pd
import xgboost as xgb
from sklearn.metrics import roc_auc_score, classification_report, precision_recall_fscore_support, accuracy_score

# --- 0) Compute class imbalance weight from TRAIN ONLY (good practice) ---
pos = int(y_train.sum())
neg = int(len(y_train) - pos)
scale_pos_weight = neg / max(pos, 1)
print(f"[XGB] scale_pos_weight (train): {scale_pos_weight:.3f}")

# --- 1) Helper to build one row of metrics (focus on class 1 = 'cancelled') ---
def metric_row(name, y_true, y_pred, y_prob):
    prec, rec, f1, _ = precision_recall_fscore_support(
        y_true, y_pred, average=None, labels=[1]
    )
    return {
        "Model": name,
        "Test ROC-AUC": roc_auc_score(y_true, y_prob),
        "Accuracy": accuracy_score(y_true, y_pred),
        "Precision (class 1)": float(prec[0]),
        "Recall (class 1)": float(rec[0]),
        "F1 (class 1)": float(f1[0]),
    }

# --- 2) Grab best params from Optuna and train tuned XGB ---
best_params = study.best_trial.params  # if your variable isn't 'study', replace here
xgb_tuned = xgb.XGBClassifier(
    objective="binary:logistic",
    eval_metric="auc",
    tree_method="hist",      # use 'gpu_hist' if you have GPU
    random_state=42,
    n_jobs=-1,
    scale_pos_weight=scale_pos_weight,
    **best_params
)
xgb_tuned.fit(X_train, y_train)
y_prob_tuned = xgb_tuned.predict_proba(X_test)[:, 1]
y_pred_tuned = xgb_tuned.predict(X_test)

print("\n[XGB tuned (Optuna)] Test ROC-AUC:", roc_auc_score(y_test, y_prob_tuned))
print("[XGB tuned (Optuna)] Classification Report:\n",
      classification_report(y_test, y_pred_tuned, digits=3))

# --- 3) Use your ALREADY-FITTED baseline model if it exists ---
baseline_rows = []
baseline_found = False

for candidate_name in ["xgb_model", "xgb_baseline"]:
    try:
        baseline = globals()[candidate_name]
        # sanity check that it looks like a fitted XGB model
        _ = baseline.get_booster()
        y_prob_base = baseline.predict_proba(X_test)[:, 1]
        y_pred_base = baseline.predict(X_test)
        print(f"\n[Using existing baseline: {candidate_name}] Test ROC-AUC:",
              roc_auc_score(y_test, y_prob_base))
        print(f"[{candidate_name}] Classification Report:\n",
              classification_report(y_test, y_pred_base, digits=3))
        baseline_rows.append(metric_row("XGB baseline", y_test, y_pred_base, y_prob_base))
        baseline_found = True
        break
    except Exception:
        continue

# --- 4) Build the comparison table (baseline vs tuned) ---
rows = baseline_rows + [metric_row("XGB tuned (Optuna)", y_test, y_pred_tuned, y_prob_tuned)]
xgb_cmp_df = pd.DataFrame(rows).sort_values("Test ROC-AUC", ascending=False).reset_index(drop=True)
print("\n===== XGBoost Comparison (Test Set) =====")
xgb_cmp_df

"""### HistGradientBoosting MODEL - Baseline"""

from sklearn.ensemble import HistGradientBoostingClassifier
from sklearn.metrics import roc_auc_score, classification_report, accuracy_score, precision_recall_fscore_support
import numpy as np
import pandas as pd

# Build per-row weights for TRAIN from your previously computed `weights` dict: {0: w0, 1: w1}
w_train = np.where(y_train == 1, weights[1], weights[0])  # uses the dict you printed earlier

# Baseline HGB
hgb_base = HistGradientBoostingClassifier(
    random_state=42,
    learning_rate=0.1,
    max_depth=None,        # let it choose by leaves
    max_leaf_nodes=31,     # light default
    max_bins=255,
    l2_regularization=0.0,
    early_stopping=True
)

# fit WITH sample weights (this is how HGB handles class imbalance)
hgb_base.fit(X_train, y_train, sample_weight=w_train)

# Evaluate on TEST (no weights needed here)
y_prob_hgb_base = hgb_base.predict_proba(X_test)[:, 1]
y_pred_hgb_base = hgb_base.predict(X_test)
print("[HGB baseline] Test ROC-AUC:", roc_auc_score(y_test, y_prob_hgb_base))
print("[HGB baseline] Classification Report:\n", classification_report(y_test, y_pred_hgb_base, digits=3))

"""### HistGradientBoosting with RandomizedSearchCV

"""

from sklearn.model_selection import RandomizedSearchCV

# Keep the same train weights
w_train = np.where(y_train == 1, weights[1], weights[0])

hgb = HistGradientBoostingClassifier(random_state=42, early_stopping=True)

param_dist = {
    "learning_rate": [0.03, 0.05, 0.1],
    "max_leaf_nodes": [15, 31, 63],
    "max_depth": [None, 4, 6],
    "min_samples_leaf": [20, 50, 100],   # larger leaves help generalization on noisy data
    "l2_regularization": [0.0, 0.1, 1.0]
}

hgb_search = RandomizedSearchCV(
    estimator=hgb,
    param_distributions=param_dist,
    n_iter=10,              # tiny search -> fast
    scoring="roc_auc",
    cv=3,
    random_state=42,
    n_jobs=-1,
    verbose=1
)

# IMPORTANT: pass the sample weights into .fit so every CV fold uses them
hgb_search.fit(X_train, y_train, **{"sample_weight": w_train})

print("[HGB tuned] Best params:", hgb_search.best_params_)
print("[HGB tuned] Best CV AUC:", round(hgb_search.best_score_, 4))

hgb_tuned = hgb_search.best_estimator_

"""### HistGradientBoosting - RandomizedSearchCV with TEST DATA"""

from sklearn.metrics import roc_auc_score, classification_report, accuracy_score, precision_recall_fscore_support

# Tuned model on TEST
y_prob_hgb_tuned = hgb_tuned.predict_proba(X_test)[:, 1]
y_pred_hgb_tuned = hgb_tuned.predict(X_test)
print("\n[HGB tuned] Test ROC-AUC:", roc_auc_score(y_test, y_prob_hgb_tuned))
print("[HGB tuned] Classification Report:\n", classification_report(y_test, y_pred_hgb_tuned, digits=3))

def row(name, y_true, y_pred, y_prob):
    prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=None, labels=[1])
    return {
        "Model": name,
        "Test ROC-AUC": roc_auc_score(y_true, y_prob),
        "Accuracy": accuracy_score(y_true, y_pred),
        "Precision (class 1)": float(prec[0]),
        "Recall (class 1)": float(rec[0]),
        "F1 (class 1)": float(f1[0]),
    }

cmp_hgb = pd.DataFrame([
    row("HGB baseline", y_test, y_pred_hgb_base, y_prob_hgb_base),
    row("HGB tuned (RandomizedSearchCV)", y_test, y_pred_hgb_tuned, y_prob_hgb_tuned),
]).sort_values("Test ROC-AUC", ascending=False).reset_index(drop=True)

print("\n===== HGB Comparison (Test Set) =====")
cmp_hgb

"""### Comparison with all 4 models

### Pick Best Model
"""

# ===== Pick a final model, prioritizing Recall & F1 for class 1 =====
# Create a dict linking names to fitted objects:
model_objects = {
    "LogReg (balanced)": lr_final,
    "RandomForest tuned": best_rf,
    "XGBoost tuned (Optuna)": xgb_tuned,
    "HistGB tuned": hgb_tuned,
}

# Rank by Recall (class 1) then F1 then AUC:
ranked = all_cmp.sort_values(
    ["Recall (class 1)", "F1 (class 1)", "Test ROC-AUC"],
    ascending=[False, False, False]
).reset_index(drop=True)

final_model_name = ranked.loc[0, "Model"]
final_model = model_objects[final_model_name]

print(f"\n Selected final model: {final_model_name}")
ranked.head(4)

"""### Front End Connection"""

# === Export artifacts for Streamlit ===
import joblib, json, os
ART_DIR = "artifacts"
os.makedirs(ART_DIR, exist_ok=True)

# 1) Choose your final model object here:
final_model = hgb_tuned

# 2) Save model + preprocessing pieces
joblib.dump(final_model, f"{ART_DIR}/final_model.pkl")
joblib.dump(imputer,     f"{ART_DIR}/imputer.pkl")

# Persist lists that define your encoding space
joblib.dump(features,     f"{ART_DIR}/features.pkl")          # final feature names used for training
joblib.dump(list(top_pickups), f"{ART_DIR}/top_pickups.pkl")  # from training
joblib.dump(list(top_drops),   f"{ART_DIR}/top_drops.pkl")

# Infer vehicle dummies you trained with (columns startwith 'vehicle_')
vehicle_cols = [c for c in features if c.startswith("vehicle_")]
joblib.dump(vehicle_cols, f"{ART_DIR}/vehicle_cols.pkl")

print("Saved artifacts to ./artifacts")

"""### Download artifacts Folder"""

import shutil
shutil.make_archive("artifacts_export", "zip", "artifacts")

from google.colab import files
files.download("artifacts_export.zip")